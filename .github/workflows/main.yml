name: Prototype to create llamafile
run-name: Create Llamafile and upload it ðŸš€
on: 
    workflow_dispatch:
env:
  # Setting an environment variable with the value of a configuration variable
  HF_TOKEN: ${{ secrets.HF_TOKEN }}
  HF_REPO: ${{ vars.HF_REPO }}
  HF_REPO_FILE: ${{ vars.HF_REPO_FILE }}
  REMOTE_GGUF_MODEL: ${{ vars.REMOTE_GGUF_MODEL }}
jobs:
  create-llamafile-and-upload:
    runs-on: ubuntu-22.04
    steps:
      - name: Install hf CLI and llamafile
        run: |
            pip install -U "huggingface_hub[cli]"
            curl -L https://github.com/Mozilla-Ocho/llamafile/releases/download/0.8.6/llamafile-0.8.6.zip -O 
            unzip llamafile-0.8.6.zip 
            rm -rf llamafile-0.8.6.zip
      - name: "Fix run-detectors/WINE"
        run: |
            sudo wget -O /usr/bin/ape https://cosmo.zip/pub/cosmos/bin/ape-$(uname -m).elf
            sudo chmod +x /usr/bin/ape
            sudo sh -c "echo ':APE:M::MZqFpD::/usr/bin/ape:' >/proc/sys/fs/binfmt_misc/register"
            sudo sh -c "echo ':APE-jart:M::jartsr::/usr/bin/ape:' >/proc/sys/fs/binfmt_misc/register"

      - name: Download gguf model
        run: curl -L $REMOTE_GGUF_MODEL -o model.gguf
      - name: Convert to llamafile
        run: ./llamafile-0.8.6/bin/llamafile-convert model.gguf
      - name: Rename converted model
        run: mv model.llamafile $HF_REPO_FILE
      - name: Upload llamafile to huggingface
        run: huggingface-cli upload $HF_REPO $HF_REPO_FILE